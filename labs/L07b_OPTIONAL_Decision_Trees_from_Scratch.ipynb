{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DaGON1W6YEL1"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from typing import Optional, List\n","import io\n","import pydotplus\n","from IPython.display import Image\n","\n","RANDOM_SEED = 0xdeadbeef"]},{"cell_type":"markdown","source":["## Introduction\n","\n","In this tutorial, we develop a basic implementation of a decision tree from scratch. \n","\n","Decision Trees are a relatively simple and interpretable type of model that can be used both for classification and regression. In this lab we will look at how they are implemented for **classification**.\n","\n","In their simplest form, a Decision Tree is a *Binary Tree* that splits the data\n","at each node into two parts, based on some criterion."],"metadata":{"id":"c7xmloKetiR1"}},{"cell_type":"markdown","source":["As always, we start out by creating some synthetic data. You should notice that this data is not linearly separable, meaning there is no line such that all red points are on one side and all the yellow points on the other.\n","\n","One nice feature of Decision Trees is that they can handle situations like this without any \"tricks\"."],"metadata":{"id":"lGB1XZIZu9Oi"}},{"cell_type":"code","source":["from sklearn.datasets import make_blobs\n","\n","X, y = make_blobs(\n","    n_samples=800,\n","    n_features=2,\n","    centers=np.array([\n","        [-1., -1.],\n","        [1., -1],\n","        [1., 1.],\n","        [-1., 1.],\n","    ]),\n","    cluster_std=0.25,\n","    random_state=RANDOM_SEED,\n",")\n","y[y == 2] = 0\n","y[y == 3] = 1\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\"autumn\", edgecolors='b')"],"metadata":{"id":"xwFSt5QiySU-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Gini Impurity\n","\n","In class you have seen the *Gini Impurity* as one possible criterion to split data points.\n","\n","The Gini Impurity is computed on a collection of class labels. Assume we are given a list of $n$ class labels. Each label can have one of $k$ discrete values. Then the Gini Impurity is computed as: $\\sum_{c = 1}^{k} p_c(1 - p_c)$. In this formula $p_c = \\frac{n_c}{n}$ is the fraction of labels of class $c$.\n","\n","In class you have seen a simplified version with only 2 classes: \"Yes\" (1) and \"No\" (0). In that case the Gini Impurity simplifies to: $1 - p_1^2 - p_0^2$."],"metadata":{"id":"u4wATXfkxmb0"}},{"cell_type":"markdown","source":["In the next cell, we implement the Gini Impurity for a given list of labels. For simplicity, we assume that each label can only have a value of either 0 or 1."],"metadata":{"id":"qMhnj1fAz7S9"}},{"cell_type":"code","source":["def gini_impurity(labels: np.array) -> float:\n","  \"\"\"\n","  Compute Gini Impurity for labels\n","\n","  labels: a 1D array of labels with values of either 0 or 1\n","\n","  return: the Gini Impurity\n","  \"\"\"\n","  n = len(labels)\n","  p0 = (labels == 0).sum() / n\n","  p1 = (labels == 1).sum() / n\n","  return 1. - p0*p0 - p1*p1"],"metadata":{"id":"BGchnz8KYqx6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When deciding where to split, we compute the a weighted average of the impurities for the left and right sub-trees: $p_{left} \\times G_{left} + p_{right} \\times G_{right}$.\n","\n","Here $G_{left}$ is the Gini Impurity of the left sub-tree and $p_{left}$ is the fraction of data assigned to the left sub-tree (and analogous for $G_{right}$ and $p_{right}$)."],"metadata":{"id":"RW1fL4vC0nwL"}},{"cell_type":"code","source":["def split_impurity(labels_left: np.array, labels_right: np.array) -> float:\n","  \"\"\"\n","  Computed the weighted average Gini Impurity for a given split of labels.\n","\n","  left_labels: 1D array of labels with values of either 0 or 1 that belong to the left sub-tree\n","  right_labels: 1D array of labels with values of either 0 or 1 that belong to the right sub-tree\n","\n","  return: weighted average of the Gini Impurities of the left and right sub-trees\n","  \"\"\"\n","  n_left = len(labels_left)\n","  n_right = len(labels_right)\n","  p_left = n_left / (n_left + n_right)\n","  p_right = n_right / (n_left + n_right)\n","  gini_left = gini_impurity(labels_left)\n","  gini_right = gini_impurity(labels_right)\n","  return p_left*gini_left + p_right*gini_right"],"metadata":{"id":"7ujeAmVRnW8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function `split_impurity` that you just implemented can be used to decide between multiple potential splits by selecting with the lowest value.\n","\n","To find the best split for numerical data you would do the following (see also the lecture slides, as a reminder):\n","* 1.  sort numeric data from lowest to highest\n","* 2.  compute average of every pair of adjacent values as candidate split threshold\n","* 3.  compute `split_impurity` for each possible split\n","* 4.  select the one with the minimum `split_impurity`\n","\n","\n","In the next cell this procedure is implemented to find the best location to split numeric data."],"metadata":{"id":"XfH81UTd2-qg"}},{"cell_type":"code","source":["def split_location(x: np.array, labels: np.array) -> float:\n","  \"\"\"\n","  Find the best split location for given numeric data\n","\n","  x: a 1D array of numeric (float) values \n","  labels: a 1D array of labels (0 or 1) associated with x\n","  \n","  return: the threshold t where to split the data, such that data points <= t go to the left sub-tree and data points > t go to the right sub-tree\n","  \"\"\"\n","  sorted = np.sort(x)\n","  threshs = (sorted[:-1] + sorted[1:]) / 2.\n","\n","  best_thresh = -1.\n","  best_score = np.inf\n","  for th in threshs:\n","    left = labels[x <= th]\n","    right = labels[x > th]\n","    score = split_impurity(left, right)\n","\n","    if score < best_score:\n","      best_thresh = th\n","      best_score = score\n","  \n","  return best_thresh"],"metadata":{"id":"eHb2dzi8hFpu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Decision Tree Implementation\n","\n","We now have everything we need to implement our own Decision Tree. In the next 2 cells we have the code for one possible implementation.\n","In the next cell we have the class `DecisionTreeNode` representing a node inside a Decision Tree. \n"],"metadata":{"id":"x_mQw1Pq_00m"}},{"cell_type":"code","source":["class DecisionTreeNode:\n","\n","  def __init__(\n","    self,\n","    data: np.array,\n","    labels: np.array,\n","    split_dimension: int,\n","    min_points_per_leaf: int,\n","  ):\n","    # a 2D array of numerical data, rows (dim 0) are samples and columns (dim 1) are features\n","    self.data = data\n","    # a 1D array of labels (0 or 1) associated with data\n","    self.labels = labels\n","    # each node can only split on 1 feature dimension\n","    self.split_dimension = split_dimension\n","    # we won't split nodes that have too few elements\n","    self.min_points_per_leaf = min_points_per_leaf\n","\n","    # left sub-tree (if not a leaf)\n","    self.left: Optional[DecisionTreeNode] = None\n","    # right sub-tree (if not a leaf)\n","    self.right: Optional[DecisionTreeNode] = None\n","    # the decision threshold for this node (if not a leaf)\n","    self.split_threshold: Optional[float] = None\n","    # if this node is a leaf, we will store its label here\n","    self.leaf_label: Optional[int] = None \n","\n","    # check whether a nude is pure, meaning there are only samples from 1 class\n","    # in that case we won't have split any further\n","    pure_node = len(set(self.labels)) == 1\n","\n","    # we won't split nodes with very few elements, this is a type of pruning\n","    # you've seen in the lecture, do you remember which one?\n","    too_small = len(self.labels) <= self.min_points_per_leaf\n","\n","    if not pure_node and not too_small:\n","      # we separated out the splitting logic\n","      self.split()\n","    else:\n","      # for leaf nodes, we store the label, which is just the majority of \n","      # the data labels\n","      self.leaf_label = np.argmax(np.bincount(self.labels))\n","\n","  def split(self):\n","    \n","    # we use the function implemented earlier to find the optimal splitting\n","    # threshold.\n","    # we use self.split_dimension as the dimension to split along\n","    self.split_threshold = split_location(\n","        x=self.data[:, self.split_dimension],\n","        labels=self.labels,\n","    )\n","\n","    # compute row indices of which data points will go to the left and right sub-trees\n","    left_ixs = (self.data[:, self.split_dimension] <= self.split_threshold)\n","    right_ixs = (self.data[:, self.split_dimension] > self.split_threshold)\n","\n","    # each child node will use a different column to split. we use modulo % to wrap around.\n","    next_split = (self.split_dimension + 1) % self.data.shape[1]\n","\n","    # initialize left / right children\n","    # we only pass the data and labels selected by the indices that we computed earlier\n","    self.left = DecisionTreeNode(\n","        data=self.data[left_ixs, :],\n","        labels=self.labels[left_ixs],\n","        split_dimension=next_split,\n","        min_points_per_leaf=self.min_points_per_leaf,\n","    )\n","    self.right = DecisionTreeNode(\n","        data=self.data[right_ixs, :],\n","        labels=self.labels[right_ixs],\n","        split_dimension=next_split,\n","        min_points_per_leaf=self.min_points_per_leaf,\n","    )\n","\n","  def predict(self, X) -> np.array:\n","    # predict labels for new, unseen data X\n","\n","    n_samples = X.shape[0]\n","\n","    if self.leaf_label is not None:\n","      # if current node is a leaf, we just return its label (one for each row in the input data)\n","      return np.ones(n_samples, dtype=int) * self.leaf_label\n","    else:\n","      # for non-leaf nodes, we have to split the data and get predictions from the child nodes\n","\n","      # initialize result variable\n","      result = np.zeros(n_samples, dtype=int)\n","\n","      # compute row indices of which data points belong to which child sub-tree\n","      left_ixs = (X[:, self.split_dimension] <= self.split_threshold)\n","      right_ixs = (X[:, self.split_dimension] > self.split_threshold)\n","\n","      # get predictions from sub-trees\n","      result[left_ixs] = self.left.predict(X[left_ixs, :])\n","      result[right_ixs] = self.right.predict(X[right_ixs, :])\n","      \n","      return result\n","    \n","  def print(self, prefix: str = \"\"):\n","    # Helper method to 'pretty' print the Decision Tree\n","    # You do not have to understand this.\n","    if self.leaf_label is not None:\n","      print(f\"{prefix} Predict {self.leaf_label}\")\n","    else:\n","      print(f\"{prefix} Dimension {self.split_dimension} <= {self.split_threshold:.3f}\")\n","      self.left.print(prefix=prefix + \"\\t\")\n","      print(f\"{prefix} Dimension {self.split_dimension} > {self.split_threshold:.3f}\")\n","      self.right.print(prefix=prefix + \"\\t\")"],"metadata":{"id":"s31m-7XGrx0S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","In the cell below, we provide a light wrapper class `CustomDecisionTree` that exposes the usual `sklearn` interface with a `.fit` and `.predict` method."],"metadata":{"id":"giifCL1_Rdpg"}},{"cell_type":"code","source":["class CustomDecisionTree:\n","\n","  def __init__(\n","    self,\n","    min_points_per_leaf: int = 5,\n","  ):\n","    self.root: Optional[DecisionTreeNode] = None\n","    self.min_points_per_leaf = min_points_per_leaf\n","  \n","  def fit(self, X, y):\n","    self.root = DecisionTreeNode(\n","        data=X,\n","        labels=y,\n","        split_dimension=0,\n","        min_points_per_leaf=self.min_points_per_leaf,\n","    )\n","    return self\n","\n","  def predict(self, X):\n","    if self.root is None:\n","      raise ValueError(f\"you have to call `fit` before you can get predictions\")\n","    return self.root.predict(X)\n","\n","  def print(self):\n","    print(\"ROOT:\")\n","    if self.root is not None:\n","      self.root.print(prefix=\"\\t\")\n","    else:\n","      print(f\"\\tnot yet fitted\")\n"],"metadata":{"id":"fQqky0aqr8pl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Question: Does our implementation apply any pruning technique? If so, which kind of pruning?*\n"],"metadata":{"id":"ImP0MjbvRukT"}},{"cell_type":"markdown","source":["### Trying it out\n","\n","Below, we apply our `CustomDecisionTree` to the initial data `X` and `y`. We will also visualize its decision boundaries.\n","\n","\n","*Note*: we provide the function `plot_decision_boundaries` in the next cell, which we will use to visualize the decision regions of different classifiers."],"metadata":{"id":"4wAZ7d0lSKOh"}},{"cell_type":"code","source":["# Helper function to visualize the decision boundaries and regions of a classifier\n","# You do not need to read it or understand how it works\n","\n","def plot_decision_boundaries(\n","  classifier,\n","  data: np.array,\n","  labels: np.array,\n","):\n","  \"\"\"\n","  Plot the decision boundaries and regions for a classifier on 2D data\n","\n","  classifier: any classifier implementing a .predict method that returns integer labels\n","  data: a 2D numpy array of shape [n_samples, 2], containing data points to be classified\n","  labels: a 1D numpy array of shape [n_samples], containing the labels for data \n","  \"\"\"\n","  x_lo = np.min(data[:, 0]) - .5\n","  x_hi = np.max(data[:, 0]) + .5\n","  y_lo = np.min(data[:, 1]) - .5\n","  y_hi = np.max(data[:, 1]) + .5\n","\n","  xx, yy = np.meshgrid(\n","      np.arange(x_lo, x_hi, 0.02),\n","      np.arange(y_lo, y_hi, 0.02),\n","  )\n","\n","  grid_preds = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n","  grid_preds = grid_preds.reshape(xx.shape)\n","\n","  plt.contourf(xx, yy, grid_preds, alpha=.6, cmap='autumn')\n","  plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='autumn', edgecolors='b')\n","\n","  plt.show()\n"],"metadata":{"id":"tNPP7ytP0psy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Have a look at the print-out of the decision tree below and the plot of it decisions on our `X` and `y`."],"metadata":{"id":"AElMlg__V9qt"}},{"cell_type":"code","source":["custom_tree = CustomDecisionTree()\n","custom_tree.fit(X, y)\n","custom_tree.print()\n","plot_decision_boundaries(custom_tree, X, y)"],"metadata":{"id":"B7nRS9zW0SLm"},"execution_count":null,"outputs":[]}]}