{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","from matplotlib import pyplot as plt\n","import keras\n","\n","RANDOM_SEED = 0xdeadbeef"],"metadata":{"id":"hFZAVpMqRMM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Neural Networks 2"],"metadata":{"id":"Deya3Rh93K_q"}},{"cell_type":"markdown","source":["# Task 1. Keras (3 Points)"],"metadata":{"id":"zK8emWp13c00"}},{"cell_type":"markdown","source":["In this task we will revisit [keras](https://keras.io).\n","We will work with the MNIST hand-written digit recognition dataset. in the first part we describe in detail how the neural network is setup. In the second part, you will train and optimize the network using dropout.\n","\n","In the next cell we will load the MNIST data and do some light preprocessing."],"metadata":{"id":"z1gDB-m_3r1_"}},{"cell_type":"code","source":["from keras.datasets import mnist\n","\n","# download official train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# the original image data are 8bit integers, we normalize them to floats in [0., 1.]\n","x_train = x_train.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255\n","\n","# the original labels are the digits 0-9\n","# that means an image of a written \"0\" has label 0.\n","# here we transform these to so-called one-hot vectors\n","# the one-hot vector has 10 dimensions (one for each class)\n","# and is all 0 except for the dimension corresponding to the label\n","# for example, label 2 would be encoded as [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","# this conversion is necessary for training with keras\n","y_train = keras.utils.to_categorical(y_train)\n","y_test = keras.utils.to_categorical(y_test)\n","\n","print(\"train samples: \", x_train.shape[0])\n","print(\"test samples: \", x_test.shape[0])\n","\n","print(\"shape of one sample: \", x_train[0].shape)"],"metadata":{"id":"7QWgU8SR4bkZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we visualize the first few training samples:"],"metadata":{"id":"mdWv1uvN6U6U"}},{"cell_type":"code","source":["for i in range(8):\n","  plt.imshow(x_train[i, :, :])\n","  plt.show()"],"metadata":{"id":"gPxjeSZI6UQB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next cell, we define a simple feed-forward neural network.\n","\n","Compared to last week there are a few changes:\n","\n","We do not use separate `Activation` layers, but instead specify the activation function using the keyword argument `activation` of the `Dense` layers.\n","\n","We explicitely define the shape of the input using an `Input` layer. The MNIST images are all 28 pixels wide and 28 pixels high.\n","\n","This week we will not yet consider the 2D nature of images but flatten them and treat them as 28x28 = 784 dimensional vectors. This is achieved by the `Flatten` layer.\n","\n","We use 10 output nodes, since we have 10 classes. The output layer uses softmax activation for classification (as seen in class)."],"metadata":{"id":"ekLlyH4_61mf"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Input, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Input(shape=(28, 28)))  # define input shape, here 28x28 images\n","model.add(Flatten())              # flatten 28x28 images to 784-dimensional vectors\n","model.add(Dense(128, activation=\"relu\"))    # hidden layer with 128 nodes and relu activation\n","model.add(Dense(10, activation=\"softmax\"))  # output layer with 10 nodes (for 10 classes) and softmax activation\n","\n","model.summary()"],"metadata":{"id":"nDeZl3dJ7BmJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next cell, we prepare the model for training. We specify `\"categorical_crossentropy\"` as our loss function. This corresponds to the loss function for multi-class classification that you have seen in class. We will `optimizer=\"sgd\"` to train using stochastic gradient descent, and report accuracy."],"metadata":{"id":"mwQ-sB3G8-6E"}},{"cell_type":"code","source":["model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"],"metadata":{"id":"U7B05zro8aAa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next cell, we fit the model on our training data. We use `batch_size=16` and train for `epochs=20` epochs.\n","\n","We additionally use the keyword argument `validation_split=.1`. This tells the training procedure to split the training data into 90% for training and 10% for validation, similarly to how we used the scikit-learn `train_test_split` function in previous assignments.\n","\n","During training, keras will report loss and accuracy on both the training and validation split for each epoch and store them in the history output object."],"metadata":{"id":"AtysZPOc99cR"}},{"cell_type":"code","source":["history = model.fit(\n","  x_train,\n","  y_train,\n","  batch_size=16,\n","  epochs=20,\n","  validation_split=.1,\n",")"],"metadata":{"id":"qJWrN9Wb8r0y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next cell we provide a helper function to plot the training and validation losses."],"metadata":{"id":"Ej0ddTRy_Xvs"}},{"cell_type":"code","source":["def plot_history(history: keras.callbacks.History):\n","  \"\"\"\n","  plot the training and validation loss for each training epoch\n","\n","  history: a History object, output of the .fit method of a keras model\n","  \"\"\"\n","  n = len(history.history['loss'])\n","  plt.plot(np.arange(n), history.history['loss'], label=\"training loss\")\n","  plt.plot(np.arange(n), history.history['val_loss'], label=\"validation loss\")\n","  plt.xticks(range(0, n + 1, 2))\n","  plt.legend()\n","  plt.show()"],"metadata":{"id":"Fz-adY2F2RHj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we plot the learning curves:"],"metadata":{"id":"F9tM_VTeAR_x"}},{"cell_type":"code","source":["plot_history(history)"],"metadata":{"id":"LdwS_7-RRUCb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we can evaluate our model on the test data.\n","The `.evaluate` method takes the test samples and labels as input and returns a list of values. The first entry in the list is the loss over the test data, the following values are the additional metrics that we defined in the `.compile` method. In our case it will return the test loss and the accuracy on the test set.\n","You can also check `model.metrics_names` to check which value corresponds to which metric."],"metadata":{"id":"GwcPEXRYAX__"}},{"cell_type":"code","source":["for metric_name, metric_value in zip(model.metrics_names, model.evaluate(x_test, y_test)):\n","  print(metric_name, f\"{metric_value:.4f}\", sep=\"\\t\")"],"metadata":{"id":"vCCBtwwuq6ly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 1a. Add a Dropout layer"],"metadata":{"id":"BmTbTrtdFNYN"}},{"cell_type":"markdown","source":["In the next cell, we redefine the same model as above. Your task is to add a `Dropout` layer. The main argument for the dropout layer is `rate` which defines the fraction of nodes that should be dropped. For example `Dropout(rate=0.5)` would drop exactly half of the nodes every time.\n","\n","**Note:** If you ever use another framework than keras, their dropout implementation might be defined such that you specify the fraction of nodes to keep."],"metadata":{"id":"1tjMW5X5BlB3"}},{"cell_type":"markdown","source":["**Task**: add a `Dropout` layer after the hidden layer and set an appropriate `rate` parameter."],"metadata":{"id":"GSg85q_fEVs8"}},{"cell_type":"code","source":["from keras.layers import Dropout\n","\n","# TODO: add a Dropout layer\n","model_dropout = Sequential()\n","model_dropout.add(Input(shape=(28, 28)))\n","model_dropout.add(Flatten())\n","model_dropout.add(Dense(128, activation=\"relu\"))\n","model_dropout.add(Dense(10, activation=\"softmax\"))\n","\n","model_dropout.summary()"],"metadata":{"id":"V3WlH4XGFMvf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below, we train the model in the same way as before."],"metadata":{"id":"E3_qoLKsEjp5"}},{"cell_type":"code","source":["model_dropout.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n","history = model_dropout.fit(\n","  x_train,\n","  y_train,\n","  batch_size=16,\n","  epochs=20,\n","  validation_split=.1,\n",")"],"metadata":{"id":"c8KvDAv-FkdB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below we plot the learning curve of the model with dropout.\n","\n","**Task**: Compare the learning curves to the model without dropout. What changed? Why?"],"metadata":{"id":"Kc7O9KbOFWnA"}},{"cell_type":"code","source":["plot_history(history)"],"metadata":{"id":"aPjrOXL0FTD4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we can evaluate our model with dropout on the test data."],"metadata":{"id":"G1U9x-34Fp9S"}},{"cell_type":"code","source":["for m, v in zip(model_dropout.metrics_names, model_dropout.evaluate(x_test, y_test)):\n","  print(m, f\"{v:.4f}\")"],"metadata":{"id":"vuJoFPg0G4yo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ“¢ **HAND-IN** ðŸ“¢: in Moodle (2 Points)\n","\n","* The learning curve plot for your model with dropout\n","* What difference did you observe to the learning curves without dropout?\n","* What is your explanation for the difference?"],"metadata":{"id":"PPO1q2apBLou"}},{"cell_type":"markdown","source":["### Task 1b. Early Stopping \"by hand\"\n","\n","In this task we will again consider the intial model without dropout. We will again plot the learning curves. Your task will be to study the learning curve and decide whether early stopping makes sense and determine at which epoch we should stop."],"metadata":{"id":"SxZ5djmwHIFM"}},{"cell_type":"markdown","source":["In the next cell, we rerun the training of our initial model without dropout. We use a few more epochs this time.\n","\n","**Note** this might take a little longer to run."],"metadata":{"id":"zklyG0SPKdaT"}},{"cell_type":"code","source":["\n","# define same model, without dropout again\n","model = Sequential()\n","model.add(Input(shape=(28, 28)))\n","model.add(Flatten())\n","model.add(Dense(128, activation=\"relu\"))\n","model.add(Dense(10, activation=\"softmax\"))\n","\n","# note: we use more epochs for this task\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n","history = model.fit(\n","  x_train,\n","  y_train,\n","  batch_size=16,\n","  epochs=40,\n","  validation_split=.1,\n",")\n"],"metadata":{"id":"_Hif-x60IGR6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Task** study the learning curves in the next plot. Does early stopping makes sense? If so, at what epoch should we have stopped and why?\n","\n","**Note** your solution will very likely look very different from other people's."],"metadata":{"id":"Wo9dT6K-LWbm"}},{"cell_type":"code","source":["plot_history(history)"],"metadata":{"id":"kDDQPdVbJQP5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ“¢ **HAND-IN** ðŸ“¢: in Moodle (1 Point)\n","\n","* the plot of the learning curve in Task 1b.\n","* should we use early stopping?\n","* at what epoch would you stop?"],"metadata":{"id":"itq_TyB8LlLp"}},{"cell_type":"markdown","source":["### Early Stopping in Keras"],"metadata":{"id":"CUrR_z18L8F3"}},{"cell_type":"markdown","source":["Keras provides an implementation of early stopping using the `EarlyStopping` class.\n","\n","We can define a parameters to control how early stopping is applied. The general idea is to stop training once a particular metric does not improve any more. We want to stop training once the validation loss does not decrease anymore. For this we set `monitor=\"val_loss\"` to tell it to check the values of the validation loss. Since we want to stop once we achive a minimal validation loss, we set `mode=\"min\"`. Finally, we set `patience=1` to tell it to stop training after the validation loss has not improved for 1 whole epoch.\n","\n","We then pass the `EarlyStopping` instance as a callback to the `.fit` method, using `callbacks=[early_stopping]`. Keras implements several functionalities using callbacks that run at the end of every epoch, early stopping is one of them."],"metadata":{"id":"px-LIw0OMlTX"}},{"cell_type":"code","source":["from keras.callbacks import EarlyStopping\n","\n","# prepare our model\n","model = Sequential()\n","model.add(Input(shape=(28, 28)))\n","model.add(Flatten())\n","model.add(Dense(128, activation=\"relu\"))\n","model.add(Dense(10, activation=\"softmax\"))\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n","\n","early_stopping = EarlyStopping(\n","    monitor=\"val_loss\",\n","    mode=\"min\",\n","    patience=1,\n",")\n","\n","# note that we pass the early_stopping object as a callback here\n","history = model.fit(\n","  x_train,\n","  y_train,\n","  batch_size=16,\n","  epochs=40,\n","  validation_split=.1,\n","  callbacks=[early_stopping]\n",")\n","\n","plot_history(history)\n"],"metadata":{"id":"K7KZCuM0HIdb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Task 2 Data Augmentation (2 Points)"],"metadata":{"id":"-fu39IZiRA6l"}},{"cell_type":"markdown","source":["In this task we will look at some image data augmentation methods that are implemented in keras.\n","\n","In the next cell we prepare the data used for this task. We will use the first 8 samples from the MNIST training data. As mentioned, the original MNIST images are 28x28 pixels and each pixel consists of 1 float value. The keras image processing functions expect inputs with shape (width, height, channels) where channels usually is 3, corresponding to red, green, and blue. We therefore have to reshape our images to have shape (28, 28, 1), we achieve this using `np.newaxis` when indexing the original data. Don't worry, you do not have to understand that last part."],"metadata":{"id":"5KMc3piLQw5B"}},{"cell_type":"code","source":["# You don't have to fully understand this next line\n","X = x_train[:8, :, :, np.newaxis]\n","\n","print(\"new data shape: \", X.shape)\n","print(\"number of samples: \", X.shape[0])\n","print(\"shape of an individual sample\", X.shape[1:])"],"metadata":{"id":"jrAsU0y-P186"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next cell, we use several classes implementing various image augmentation strategies.\n","\n","Since they all involve a degree of randomness, we can set their random seed using `seed=RANDOM_SEED`.\n","\n","The first augmentation we can use is `RandomFlip`. As the name suggests, it will randomly flip a given input image. We can set `mode` to any of: `\"horizontal\"`, `\"vertical\"`, or `\"horizontal_and_vertical\"`.\n","\n","Next, we can use `RandomRotation` which will rotate the input image by a random amount. The maximal angle of the rotation is defined by the `factor` parameter. It represents the maximal fraction of $2\\pi$ that we will rotate in either direction. Since rotating an image can move certain pixels outside of the initial 28x28 grid and leave certain pixels empty, we have to define what to do with the empty space. We can define this by setting `fill_mode`. Here we chose `fill_mode=\"constant\"` and a `fill_value=0.0` to fill empty pixels with 0. There are other fill-modes available but you do not have to worry about that this week.\n","\n","The last augmentation we will look at is `RandomZoom`. It has both a `height_factor` and `width_factor` which defines how much we can zoom in or out in either dimension. We again provide `fill_mode=\"constant\"` and `fill_value=0.` for the same reasons."],"metadata":{"id":"u00DxOqBSnQn"}},{"cell_type":"markdown","source":["**Task** the values of the three augmentation layers are intentionally set too extreme. Your task is to find good values for:\n","* `mode` of `RandomFlip`\n","* `factor` of `RandomRotation`\n","* `height_factor` and `width_factor` of `RandomZoom`\n","\n","There is one layer which does not make much sense for the digit recognition task. You can remove it."],"metadata":{"id":"7wRVoJwZWsfD"}},{"cell_type":"code","source":["from keras.layers import RandomFlip, RandomRotation, RandomZoom\n","\n","augmentation_layer = Sequential()\n","augmentation_layer.add(Input(shape=(28, 28, 1)))\n","augmentation_layer.add(RandomFlip(mode=\"horizontal_and_vertical\", seed=RANDOM_SEED))\n","augmentation_layer.add(RandomRotation(factor=.5, seed=RANDOM_SEED, fill_mode=\"constant\", fill_value=0.))\n","augmentation_layer.add(RandomZoom(height_factor=1., width_factor=1., fill_mode=\"constant\", fill_value=0.))"],"metadata":{"id":"GvtqSR3qRSnw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next cell, we apply the augmentations to our samples. Note that we pass the parameter `training=True`, since data augmentation is only applied during training but not testing (similar to dropout)."],"metadata":{"id":"Vw-oElwOSrFI"}},{"cell_type":"code","source":["x_aug = augmentation_layer(X, training=True)"],"metadata":{"id":"qL_OPUcJS8VB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next cell, we plot all 8 example images and their augmented versions."],"metadata":{"id":"eRQZ94hDXgMB"}},{"cell_type":"code","source":["for i in range(8):\n","  ax1 = plt.subplot(1, 2, 1)\n","  ax1.imshow(X[i, :, :, 0])\n","  ax2 = plt.subplot(1, 2, 2)\n","  ax2.imshow(x_aug[i, :, :, 0])\n","  plt.show()"],"metadata":{"id":"QjjSQlP3TCxw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ“¢ **HAND-IN** ðŸ“¢: in **Moodle** whether you solved this task"],"metadata":{"id":"3WsbJA5jXxaV"}},{"cell_type":"markdown","source":["# Task 3. The Deep End (5 Points)"],"metadata":{"id":"S1ZsKmJ7X7gc"}},{"cell_type":"markdown","source":["In this task you are on your own. We will provide you with a dataset and you will have to implement your own feed-forward neural network using keras."],"metadata":{"id":"f5psDZ4_X-eb"}},{"cell_type":"markdown","source":["The dataset we will use is the [Boston Housing Dataset](http://lib.stat.cmu.edu/datasets/boston). The goal is to predict the median value of houses in different areas around Boston based on 13 different features.\n","\n","The full list and explanation of features is given below. The last entry, \"MEDV\", is our target variable."],"metadata":{"id":"5mA-7NoVjJFX"}},{"cell_type":"markdown","source":["Variables in order:\n","* CRIM     per capita crime rate by town\n","* ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n","* INDUS    proportion of non-retail business acres per town\n","* CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n","* NOX      nitric oxides concentration (parts per 10 million)\n","* RM       average number of rooms per dwelling\n","* AGE      proportion of owner-occupied units built prior to 1940\n","* DIS      weighted distances to five Boston employment centres\n","* RAD      index of accessibility to radial highways\n","* TAX      full-value property-tax rate per \\$10,000\n","* PTRATIO  pupil-teacher ratio by town\n","* B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n","* LSTAT    % lower status of the population\n","* MEDV     Median value of owner-occupied homes in \\$1000's"],"metadata":{"id":"aV7GI3V_XgZE"}},{"cell_type":"code","source":["from keras.datasets import boston_housing\n","\n","(x_train, y_train), (x_test, y_test) = boston_housing.load_data(test_split=.2, seed=RANDOM_SEED)\n","\n","print(\"training samples: \", x_train.shape[0])\n","print(\"test samples: \", x_test.shape[0])\n","\n","print(\"sample dimension: \", x_train.shape[1])"],"metadata":{"id":"GR6VSlZ3X_nz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task: Implement a Neural Network for the Boston Housing dataset using keras"],"metadata":{"id":"DUmd3VVykRn1"}},{"cell_type":"markdown","source":["Some general advice:\n","\n","* you can a reuse a lot of the code from Task 1\n","* think about the type of problem you are solving (classification, regression, etc.) and choose the right loss function and metric(s)\n","* the number of input and output nodes of your network are defined by the data itself, the number and sizes of the hidden layers are up to you\n","* if you run into issues, where the loss becomes `nan` you can try changing the activation function"],"metadata":{"id":"8BnIB0Fwkife"}},{"cell_type":"code","source":["# Have fun (="],"metadata":{"id":"R0pDPIsUv3JG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ“¢ **HAND-IN** ðŸ“¢: in **Moodle**\n","\n","* a description of the network you built\n","* what loss function and metric(s) you used and why you chose those\n","* the final performance you achieved on the test set"],"metadata":{"id":"hpQ6ddUCkL6Y"}}]}